{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Use Vertex AI Vector Search to Recommend Similar Products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a74aaf1481"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use the Vertex AI Vector Search Service. It is a high-scale, low-latency solution, to find similar vectors (or more specifically \"embeddings\") for a large dataset. Moreover, it is a fully managed offering, further reducing operational overhead. It is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Terminology\n",
        "\n",
        "This section reviews some key terminology related to vector search.\n",
        "\n",
        "- **Vector**: A vector is a mathematical object that has both a magnitude and a direction. It is often represented as an array of numbers. In the context of machine learning, a vector is often used to represent a feature or an embedding.\n",
        "- **Embedding**: An embedding is a mathematical representation of an object, such as a word, sentence, image, or sound. In the context of machine learning, an embedding is often used to represent a feature. For this lab, product embeddings are used to represent products. Each product embedding is a 768-dimension vector meaning 768 numbers are used to represent the product.\n",
        "- **Vector Search**: Vector search is the process of finding similar vectors (or embeddings) for a given query vector. It is often used to find or recommend similar products, images, or documents.\n",
        "- **Index**: An index is a data structure that is used to store and retrieve embeddings. It is optimized for fast search and retrieval of similar embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a4b245e795"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to create Approximate Nearest Neighbor (ANN) Index in Vertex AI (formerly known as AI Platform), query against indexes, and validate the performance of the index. \n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "* Create an Index\n",
        "* Create an IndexEndpoint\n",
        "* Deploy an Index to an IndexEndpoint\n",
        "* Perform a vector search query\n",
        "* Compare the results with the ground truth\n",
        "\n",
        "The following diagram illustrates the different services involved:\n",
        "\n",
        "![](assets/overview.png)\n",
        "\n",
        "- The product embeddings are stored in Google Cloud Storage. The embeddings have been generated using [Vertex AI embeddings for text](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings) using the product names\n",
        "- Each Vector Search Index is created in Vertex AI using the embeddings.\n",
        "- There are two indexes created, one using the Approximate Nearest Neighbor algorithm (ANN) and the other using the Brute Force algorithm.\n",
        "    - Nearest neighbor refers to finding the closest embedding to a given query embedding.\n",
        "    - The ANN algorithm is optimized for fast search and retrieval of similar embeddings although it is not guaranteed to find the best solution, especially for large datasets. \n",
        "    - The Brute Force algorithm will find the true nearest neighbor but is less efficient and not recommended for production. By comparing the results of the ANN and Brute Force algorithms, you can validate the performance of the ANN algorithm.\n",
        "- To query the indexes, an IndexEndpoint is created and the indexes are deployed to the endpoint.\n",
        "- IndexEndpoints can be deployed in private/VPC or public mode. In this lab, the IndexEndpoints are deployed in private mode which is the most performant and requires VPC peering to query the endpoints. Public mode endpoints are not open to the public and still have IAM security controls available. The lab environment has been set up with VPC peering so the notebook instance can query the endpoints.\n",
        "\n",
        "*Note*: The index creation and deployment can take up to 30 minutes to complete. Because of this, they have automatically been deployed when you started this lab to save you time waiting. You will understand all of the code involved in creating and deploying the index before performing real-time queries against the index endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the sample data\n",
        "\n",
        "In this lab, you will use [TheLook dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce) which has [products](bigquery-public-data.thelook_ecommerce.products) table with about 30,000 rows of synthetic product data for a fictitious e-commerce clothing site.\n",
        "\n",
        "![](assets/Thelook.png)\n",
        "\n",
        "From this table, a `product-embeddings.json` file has been prepared for this lab which includes 5000 product embeddings.\n",
        "\n",
        "This file is in JSONL (JSON lines) format and each row has an `id` for the product id, `name` for the product name, and `embedding` for the embedding vector of the product name in 768 dimensions which was generated using [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings). A sample row from the file is as follows (where the `embedding` vector shows only 3 of the 768 dimensions):\n",
        "\n",
        "```json\n",
        "{\"id\":\"19536\",\"name\":\"original penguin men's pro-bro mock sweater\",\"embedding\":[0.015607465989887714,0.0183266568928957,0.080682516098022461,...]}\n",
        "```\n",
        "\n",
        "The text embeddings represent the meaning of the clothing product names. In this lab, you will use Vertex AI Vector Search to complete a [semantic search](https://en.wikipedia.org/wiki/Semantic_search) of the items. This sample code can be used as a basis for other simple recommendation systems where you can quickly find other items similar to a given one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f1bea346db"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following Python packages required to execute this notebook:\n",
        "\n",
        "- `google-cloud-aiplatform` - The official Python client library for Vertex AI.\n",
        "- `google-cloud-storage` - The official Python client library for Google Cloud Storage.\n",
        "- `grpcio-tools` - The gRPC tools for Python. gRPC is a high-performance, open-source universal remote procedure call (RPC) framework.\n",
        "\n",
        "You can ignore any pip errors about other dependencies as they do not impact this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfbccc635a17"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform==1.42.1 \\\n",
        "                         google-cloud-storage \\\n",
        "                         grpcio-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd28c9e4f067"
      },
      "source": [
        "## Before you begin\n",
        "#### Getting your project ID and Number from `gcloud`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80c0215f05a0"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = ! gcloud config get project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "\n",
        "# Retrieve the project number\n",
        "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4962667eec8e"
      },
      "source": [
        "### Configuring the VPC network variable\n",
        "\n",
        "To reduce any network overhead that might lead to unnecessary increase in overhead latency, it is best to call the ANN endpoints from your VPC via a direct [VPC Peering](https://cloud.google.com/vertex-ai/docs/general/vpc-peering) connection. The lab's notebook is within a VPC named `ai-net` which is peered with Google private services VPC to query the private Vertex AI endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDH8CgQiSxhv"
      },
      "outputs": [],
      "source": [
        "VPC_NETWORK = \"ai-net\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Configuring the Cloud Storage bucket\n",
        "\n",
        "The lab environment created a storage bucket to store the embeddings used for building the Vector Search indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"{BUCKET_PLACEHOLDER}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6Wwv-hCCN-"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "Download the product embedding dataset to the notebook environment. You will use the data set to retrieve product titles for the product IDs returned by the vector search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! gsutil cp \"gs://ca-lab-assets/vector-search/product-embeddings.json\" ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy the data to the lab's Cloud Storage bucket where it is accessible to the Vertex AI services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! gsutil cp \"gs://ca-lab-assets/vector-search/product-embeddings.json\" \"$BUCKET_URI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mglUPwHpJH98"
      },
      "source": [
        "## Creating Indexes\n",
        "\n",
        "Now everything is ready to load the embeddings to Vector Search indexex. The Vector Search APIs are available in the [aiplatform](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform) package of the Google SDK (Vertex AI was formerly known as AI Platform). \n",
        "\n",
        "You will begin by reviewing the code to create the ANN index and then create the brute force index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location='us-central1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a [MatchingEngineIndex](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex) with its `create_tree_ah_index` function (Matching Engine is the previous name of Vector Search)."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# create ANN Index\n",
        "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=\"product-index\",\n",
        "    contents_delta_uri=BUCKET_URI,\n",
        "    dimensions=768,\n",
        "    approximate_neighbors_count=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By calling the `create_tree_ah_index` function, it starts building an Index. Note that the code above does not run since the index has already been created for you when the lab started. The index takes a few minutes to create for small datasets (such as the product embeddings in this lab), otherwise an hour or more can be expected depending on the data size. You can check status of the index creation on the Vector Search Console **INDEXES** tab:\n",
        "\n",
        "- In the Google Cloud Console search bar, enter *vector search* and click the **Vector Search** result to open the Vertex AI Vector Search indexes view:\n",
        "\n",
        "![](assets/indexes.png)\n",
        "\n",
        "#### Function parameters\n",
        "\n",
        "- `contents_delta_uri`: The URI of Cloud Storage directory where you stored the embedding JSON files\n",
        "- `dimensions`: Dimension size of each embedding. In this case, it is 768 as you are using the embeddings from the Text Embeddings API.\n",
        "- `approximate_neighbors_count`: how many similar items you want to retrieve in typical cases\n",
        "\n",
        "See [the documentation](https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index) for more details on creating the Index and the parameters.\n",
        "\n",
        "The following code creates the brute force index."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# create brute force index\n",
        "index_brute_force = aiplatform.MatchingEngineIndex.create_brute_force_index(\n",
        "    display_name=\"product-index-brute-force\",\n",
        "    contents_delta_uri=BUCKET_URI,\n",
        "    dimensions=768\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Index Endpoints\n",
        "\n",
        "To use an Index, you need to create an [Index Endpoint](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public). It works as a server instance accepting query requests for your Index."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=f\"product-index-endpoint-cal\", public_endpoint_enabled=True\n",
        ")\n",
        "\n",
        "index_brute_force_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=f\"product-index-brute-force-endpoint-cal\", public_endpoint_enabled=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that it is possible to deploy multiple indexes to the same index endpoint. However two are used in this lab to avoid any confusion.\n",
        "\n",
        "In the Google Cloud Console, you can see the Index Endpoints you have created by navigating to the Vector Search Console **INDEX ENDPOINTS** tab:\n",
        "\n",
        "![](assets/indexendpoints.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Deploying the Indexes to the Index Endpoints\n",
        "\n",
        "With the Index Endpoints available, deploy the Indexes by specifying unique deployed index IDs."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "index_endpoint.deploy_index(index=index, deployed_index_id=\"product_deployed_index\")\n",
        "index_brute_force_endpoint.deploy_index(index=index, deployed_index_id=\"product_brute_force_deployed_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploying the indexes can take <ins>up to 40 minutes</ins> to complete (although it may take as little as 10 minutes) and this is the main reason why the lab deploys them for you. <ins>You must ensure the indexes are deployed before querying them in the next section</ins>.\n",
        "\n",
        "Return to the Vector Search Console's **INDEX ENDPOINTS** tab and periodically refresh the page to check the status of the index deployment (you must refresh the page to see the status change). When the indexes are deployed you will see green checkmarks in the **Deployed indexes** column:\n",
        "\n",
        "![](assets/deployedindexes.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The indexes must finish deploying before proceeding. Please wait until the Deployed indexes green check marks appear before continuing. Remember to refresh the page every few minutes. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performing a vector search query\n",
        "\n",
        "Now that the indexes are deployed, you can perform a vector search query. The following code performs a vector search query using the ANN index first and then the brute force index for comparison.\n",
        "\n",
        "### Getting an embedding to run a query\n",
        "\n",
        "First, load the embedding JSON file to build a dictionary of product names and embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# build dictionaries for product names and embeddings\n",
        "product_names = {}\n",
        "product_embeddings = {}\n",
        "with open(\"product-embeddings.json\") as f:\n",
        "    for l in f.readlines():\n",
        "        p = json.loads(l)\n",
        "        id = p[\"id\"]\n",
        "        product_names[id] = p[\"name\"]\n",
        "        product_embeddings[id] = p[\"embedding\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the `product_embeddings` dictionary, you can specify a product ID to get an embedding for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the embedding for ID 12711 \"hurley juniors supersuede beachrider boardshort\"\n",
        "# you can also try with other IDs such as 18090, 19536 and 11863\n",
        "query_embedding = product_embeddings[\"12711\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the index endpoints were created for you when the lab started, you will need to retrieve them before use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_endpoints = aiplatform.MatchingEngineIndexEndpoint.list()\n",
        "index_endpoint = None\n",
        "index_brute_force_endpoint = None\n",
        "for i in index_endpoints:\n",
        "    if \"brute-force\" not in i.display_name:\n",
        "        index_endpoint = i\n",
        "    else:\n",
        "        index_brute_force_endpoint = i\n",
        "\n",
        "index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "    index_endpoint_name=index_endpoint.resource_name\n",
        ")\n",
        "index_brute_force_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "    index_endpoint_name=index_brute_force_endpoint.resource_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can query the index endpoints using the query embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3KYVw5HB-4v"
      },
      "outputs": [],
      "source": [
        "# Test query\n",
        "\n",
        "NUM_NEIGHBOURS = 10 # The number of nearest neighbors to be retrieved\n",
        "\n",
        "# Execute the request\n",
        "response = index_endpoint.find_neighbors(\n",
        "    deployed_index_id='product_deployed_index',\n",
        "    queries=[query_embedding],\n",
        "    num_neighbors=NUM_NEIGHBOURS,\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output displays the raw response which includes the product IDs and distances to similar products. Note that the first product is the same product used for querying (ID 12711). The distances are the cosine similarity between the query embedding and the similar product embeddings. With cosine similarity a distance of 1 means the embeddings are equal and -1 means they are opposite. The feature_vector fields are all `None` because the query did not request to include them in the response.\n",
        "\n",
        "Using the IDs you can retrieve the product names from the `product_names` dictionary and confirm that the approximate nearest neighbors are indeed similar to the query product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show the results\n",
        "for idx, neighbor in enumerate(response[0]):\n",
        "    print(f\"{neighbor.distance:.2f} {product_names[neighbor.id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeUZO3bAGoM-"
      },
      "source": [
        "### Comparing to the ground truth\n",
        "\n",
        "Use the deployed brute force index as the ground truth to calculate the recall of ANN Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9dNIbkEGoM-"
      },
      "outputs": [],
      "source": [
        "# Execute the request\n",
        "brute_force_response = index_brute_force_endpoint.find_neighbors(\n",
        "    deployed_index_id='product_brute_force_deployed_index',\n",
        "    queries=[query_embedding],\n",
        "    num_neighbors=NUM_NEIGHBOURS,\n",
        ")\n",
        "for idx, neighbor in enumerate(brute_force_response[0]):\n",
        "    print(f\"{neighbor.distance:.2f} {product_names[neighbor.id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe that the more efficient ANN index is able to find the true nearest neighbors for the query product. You may repeat the comparison with other query embeddings if you have time remaining in your lab session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you demonstrated how to use the Vertex AI Vector Search to build a product similarity search system.\n",
        "Along the way you learned about the following Vertex AI resources:\n",
        "\n",
        "- Vector Search Indexes\n",
        "- Vector Search Index Endpoints\n",
        "- Vector Search Deployed Indexes\n",
        "\n",
        "Although not explored in this notebook, the indexes can be updated by providing incremental JSONL files to the `update_embeddings` method. \n",
        "This allows for the addition of new embeddings to the index without the need to re-create the index from scratch.\n",
        "\n",
        "Return to the Cloud Academy Lab page to complete the lab."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk_matching_engine_for_indexing.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
